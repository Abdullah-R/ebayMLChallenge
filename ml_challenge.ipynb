{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "from transformers import TFBertModel\n",
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow logging (1 = INFO, 2 = WARNING, 3 = ERROR)\n",
    "tf.get_logger().setLevel('ERROR')         # Suppress TensorFlow logging\n",
    "\n",
    "model_name = \"bert-base-german-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "base_bert_model = TFBertModel.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq = 60\n",
    "data_path = \"Train_Tagged_Titles.tsv\"\n",
    "df = pd.read_csv(data_path, sep=\"\\t\", dtype=str, keep_default_na=False, na_values=[\"\"], quoting=csv.QUOTE_NONE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row, last_non_nan_entity):\n",
    "    if pd.isna(row['Tag']):\n",
    "        if last_non_nan_entity is not None:\n",
    "            return 'I-' + last_non_nan_entity[0]\n",
    "        else:\n",
    "            return row['Tag']\n",
    "    else:\n",
    "        last_non_nan_entity[0] = row['Tag']\n",
    "        return 'B-' + row['Tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to keep track of the last non-NaN entity\n",
    "last_non_nan_entity = [None]\n",
    "\n",
    "# Use apply with a lambda function\n",
    "df['mod_Tag'] = df.apply(lambda row: process_row(row, last_non_nan_entity), axis=1)\n",
    "\n",
    "# Check the result\n",
    "df_entities = df[['Record Number','Token','mod_Tag']]\n",
    "vocab = ['[PAD]'] + df_entities['mod_Tag'].unique().tolist()\n",
    "voc_map = {}\n",
    "for label in vocab:\n",
    "    voc_map[label] = len(voc_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = df['Title'].unique().tolist()\n",
    "\n",
    "token_ids = np.zeros(shape=(len(train_seq), max_seq), dtype=np.int32)\n",
    "\n",
    "for i, text in enumerate(train_seq):\n",
    "    encoded = tokenizer.encode(text)\n",
    "    token_ids[i, 0:len(encoded)] = encoded\n",
    "\n",
    "attention_masks = (token_ids != 0).astype(np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entities['Tokenized_Length'] = df_entities['Token'].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "\n",
    "# Group the DataFrame by 'Record Number'\n",
    "grouped_entities = df_entities.groupby('Record Number')\n",
    "\n",
    "token_labels = np.zeros(shape=(len(train_seq), max_seq), dtype=np.int32)\n",
    "\n",
    "for i in range(5000):\n",
    "    if str(i + 1) in grouped_entities.groups:\n",
    "        curr_entities = grouped_entities.get_group(str(i + 1))\n",
    "        pointer = 1\n",
    "        for _, row in curr_entities.iterrows():\n",
    "            token_len = row['Tokenized_Length']\n",
    "            token_labels[i, pointer:(pointer + token_len)] = np.array([voc_map[row['mod_Tag']]])\n",
    "            pointer += token_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenLevelF1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, name='f1_score', **kwargs):\n",
    "        super(TokenLevelF1Score, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.true_positives = self.add_weight(name='tp', shape=(num_classes,), initializer='zeros')\n",
    "        self.false_positives = self.add_weight(name='fp', shape=(num_classes,), initializer='zeros')\n",
    "        self.false_negatives = self.add_weight(name='fn', shape=(num_classes,), initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "      y_pred = tf.argmax(y_pred, axis=-1)\n",
    "      y_true = tf.cast(y_true, 'int32')\n",
    "      y_pred = tf.cast(y_pred, 'int32')\n",
    "\n",
    "      for class_id in range(self.num_classes):\n",
    "          y_true_class = tf.equal(y_true, class_id)\n",
    "          y_pred_class = tf.equal(y_pred, class_id)\n",
    "\n",
    "          tp = tf.reduce_sum(tf.cast(tf.logical_and(y_true_class, y_pred_class), 'int32'))\n",
    "          fp = tf.reduce_sum(tf.cast(y_pred_class, 'int32')) - tp\n",
    "          fn = tf.reduce_sum(tf.cast(y_true_class, 'int32')) - tp\n",
    "\n",
    "          self.true_positives.assign_add(tf.cast(tf.scatter_nd([[class_id]], [tp], shape=[self.num_classes]), 'float32'))\n",
    "          self.false_positives.assign_add(tf.cast(tf.scatter_nd([[class_id]], [fp], shape=[self.num_classes]), 'float32'))\n",
    "          self.false_negatives.assign_add(tf.cast(tf.scatter_nd([[class_id]], [fn], shape=[self.num_classes]), 'float32'))\n",
    "\n",
    "\n",
    "    def result(self):\n",
    "        precision = tf.math.divide_no_nan(self.true_positives, self.true_positives + self.false_positives)\n",
    "        recall = tf.math.divide_no_nan(self.true_positives, self.true_positives + self.false_negatives)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
    "        return tf.reduce_mean(f1)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(tf.zeros_like(self.true_positives))\n",
    "        self.false_positives.assign(tf.zeros_like(self.false_positives))\n",
    "        self.false_negatives.assign(tf.zeros_like(self.false_negatives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, val_ids, train_labels, val_labels = train_test_split(token_ids, token_labels, test_size=0.05, random_state=42)\n",
    "\n",
    "class EntityNamingModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, entity_labels=None,\n",
    "                dropout_prob=0.1):\n",
    "        super().__init__(name=\"entity_namer\")\n",
    "\n",
    "        self.bert = base_bert_model\n",
    "        self.dropout = Dropout(dropout_prob)\n",
    "        self.entity_classifier = Dense(entity_labels,\n",
    "                                     name=\"entity_classifier\")\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "\n",
    "        tokens_output, _ = self.bert(inputs, **kwargs, return_dict=False)\n",
    "\n",
    "        tokens_output = self.dropout(tokens_output, training=kwargs.get(\"training\", False))\n",
    "        entity_logits = self.entity_classifier(tokens_output)\n",
    "\n",
    "        return entity_logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = []\n",
    "c=0\n",
    "for i in [(5,0.8), (5, 0.7), (5, 0.6), (4,0.5), (3, 0.4)]:\n",
    "    model = EntityNamingModel(entity_labels=len(voc_map), dropout_prob=i[1])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=3e-5, epsilon=1e-08),\n",
    "                        loss=[SparseCategoricalCrossentropy(from_logits=True)],\n",
    "                        metrics=[SparseCategoricalAccuracy('accuracy'), TokenLevelF1Score(num_classes = len(voc_map))],\n",
    "                        run_eagerly=True)\n",
    "\n",
    "    with tf.device(\"/cpu:0\"): #running with gpu on m3 mac causes training deficiencies\n",
    "        history = model.fit(train_ids, train_labels,\n",
    "            validation_data=(val_ids, val_labels),\n",
    "            epochs=i[0], batch_size=4)\n",
    "        histories.append(history)\n",
    "    \n",
    "    loss, accuracy, f1_score = model.evaluate(val_ids,val_labels)\n",
    "    print(\"Model Param: {i}\")\n",
    "    print(\"Test Loss: {loss}\")\n",
    "    print(\"Test Accuracy: {accuracy}\")\n",
    "    print(\"Test F1_Score: {f1_score}\")\n",
    "    print(\" ------ \")\n",
    "    model.save('model_{c}.keras')\n",
    "    c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_11-22-23_9561_6070.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(text, entity_names):\n",
    "    inputs = tf.constant(tokenizer.encode(text))[None, :]\n",
    "    outputs = model(inputs)\n",
    "    entity_logits = outputs\n",
    "    entity_ids = entity_logits.numpy().argmax(axis=-1)[0, 1:-1]\n",
    "    print(\"## Entities:\")\n",
    "    for token, entity_id in zip(tokenizer.tokenize(text), entity_ids):\n",
    "        print(f\"{token:>10} : {entity_names[entity_id]}\")\n",
    "\n",
    "rev_map = {v: k for k,v in voc_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(df[\"Title\"][0], rev_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
