{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD, AdamW\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoConfig\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global variables and load data\n",
    "max_seq = 75\n",
    "data_path = \"Train_Tagged_Titles.tsv\"\n",
    "df = pd.read_csv(data_path, sep=\"\\t\", dtype=str, keep_default_na=False, na_values=[\"\"], quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dunction to add additional classes\n",
    "def process_row(row, last_an_entity):\n",
    "    if pd.isna(row['Tag']):\n",
    "        if last_an_entity is not None:\n",
    "            return 'I-' + last_an_entity[0]\n",
    "        else:\n",
    "            return row['Tag']\n",
    "    else:\n",
    "        last_an_entity[0] = row['Tag']\n",
    "        return 'B-' + row['Tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag data with new classes\n",
    "# Initialize a list to keep track of the last non-NaN entity\n",
    "last_non_nan_entity = [None]\n",
    "\n",
    "# Use apply with a lambda function\n",
    "df['mod_Tag'] = df.apply(lambda row: process_row(row, last_non_nan_entity), axis=1)\n",
    "\n",
    "# Check the result\n",
    "df_entities = df[['Record Number','Token','mod_Tag']]\n",
    "vocab = ['[PAD]'] + df_entities['mod_Tag'].unique().tolist()\n",
    "voc_map = {}\n",
    "for label in vocab:\n",
    "    voc_map[label] = len(voc_map)\n",
    "\n",
    "rev_map = {v: (k[2:] if (k[:2]=='B-') else '' ) for k,v in voc_map.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class models\n",
    "class EntityNamingModel(nn.Module):\n",
    "\n",
    "    def __init__(self, model_source = './bert_for_ebay'):\n",
    "        super(EntityNamingModel, self).__init__()\n",
    "\n",
    "        self.bert = BertForTokenClassification.from_pretrained(model_source, num_labels=len(voc_map))\n",
    "\n",
    "    def forward(self, input_id, mask, label):\n",
    "\n",
    "        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class RobertaNamingModel(nn.Module):\n",
    "\n",
    "    def __init__(self, config, dropout = 0.1, num_labels = 70):\n",
    "        super(RobertaNamingModel, self).__init__()\n",
    "\n",
    "        # Adds Roberta followed by a classifier layer\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self, input_id, mask, labels):\n",
    "        # Feed forward\n",
    "        logits = self.roberta(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
    "        logits = self.dropout(logits[0])\n",
    "        logits = self.classifier(logits)\n",
    "\n",
    "        # Generate Losses\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1 , self.num_labels) , labels.view(-1))\n",
    "\n",
    "        \n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines DataSets for training ebay listings\n",
    "class DataSeq(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, text, labels):\n",
    "\n",
    "        self.labels = labels\n",
    "        self.texts = tokenizer(text,padding='max_length', max_length = max_seq, \n",
    "                       truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_data(self, idx):\n",
    "        batch_data = {key: value[idx] for key, value in self.texts.items()}\n",
    "        return batch_data\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        return torch.LongTensor(self.labels[idx])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_data(idx)\n",
    "        batch_labels = self.get_batch_labels(idx)\n",
    "        return batch_texts, batch_labels\n",
    "    \n",
    "# Define function for adding a progress bar while tokenizing\n",
    "def tokenizer_with_progress(large_batch):\n",
    "    tokenized_texts = {'input_ids' : [],\n",
    "                       'attention_mask' : [],\n",
    "                       }\n",
    "    for text in tqdm(large_batch, desc=\"Tokenizing\", unit=\"texts\"):\n",
    "        for k,v in tokenizer(text,padding='max_length', max_length = max_seq, \n",
    "                       truncation=True, return_tensors=\"pt\").items():\n",
    "            tokenized_texts[k].append(v[0])\n",
    "        \n",
    "    for k,v in tokenized_texts.items():\n",
    "        if(v != []):\n",
    "            tokenized_texts[k] = torch.stack(v)\n",
    "    return tokenized_texts\n",
    "\n",
    "# Define datasets for inference on ebay listings\n",
    "class DataInf(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, text):\n",
    "        self.texts = tokenizer_with_progress(text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts['input_ids'])\n",
    "\n",
    "    def get_batch_data(self, idx):\n",
    "        batch_data = {key: value[idx] for key, value in self.texts.items()}\n",
    "        return batch_data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_data(idx)\n",
    "        return batch_texts\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT (Bidirectional Encoder Representations from Transformers)\n",
    "- **Release Year**: 2018\n",
    "- **Key Features**:\n",
    "  - Utilizes the Transformer architecture.\n",
    "  - Pre-trained on a large corpus of unlabelled text including the entire Wikipedia (2,500M words) and Book Corpus (800M words).\n",
    "  - Uses Masked Language Model (MLM) and Next Sentence Prediction (NSP) for training.\n",
    "- **Applications**: Sentiment analysis, question answering, language inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert specific globals\n",
    "model_name = \"bert-base-german-cased\"\n",
    "model_source = \"./bert_for_ebay\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = df['Title'].unique().tolist()\n",
    "\n",
    "df_entities['Tokenized_Length'] = df_entities['Token'].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "\n",
    "# Group the DataFrame by 'Record Number'\n",
    "grouped_entities = df_entities.groupby('Record Number')\n",
    "\n",
    "# Initialize a numpy array with zeros for label ids\n",
    "token_labels = np.zeros(shape=(len(train_seq), max_seq), dtype=np.int32)\n",
    "\n",
    "for i in range(5000):\n",
    "    if str(i + 1) in grouped_entities.groups:\n",
    "        curr_entities = grouped_entities.get_group(str(i + 1))\n",
    "        pointer = 1\n",
    "        for _, row in curr_entities.iterrows():\n",
    "            token_len = row['Tokenized_Length']\n",
    "            token_labels[i, pointer:(pointer + token_len)] = np.array([voc_map[row['mod_Tag']]])\n",
    "            pointer += token_len\n",
    "\n",
    "# Split data into train and validation sets\n",
    "train_ids, val_ids, train_labels, val_labels = train_test_split(train_seq, token_labels, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LEARNING_RATE = 5.0e-5\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# Train Loop\n",
    "def train_loop(model, train_ids, train_labels, val_ids, val_labels):\n",
    "  # Initialize a dictionary to store checkpoints in training\n",
    "  checkpoints = {\n",
    "    'epoch': [],\n",
    "    'model_state_dict': [],\n",
    "    'optimizer_state_dict': [],\n",
    "    'loss': [],\n",
    "    'accuracy': [],\n",
    "  }\n",
    "\n",
    "  train_dataset = DataSeq(train_ids, train_labels)\n",
    "  val_dataset = DataSeq(val_ids, val_labels)\n",
    "\n",
    "  train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "  val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "  # Set device to Metal Performance Shaders\n",
    "  device = torch.device(\"mps\")\n",
    "\n",
    "  # Initialize optimizer\n",
    "  optimizer = SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "  for epoch_num in range(EPOCHS):\n",
    "\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "    model.to(device)  # Move model to GPU\n",
    "    model.train()     # Set model to training mode\n",
    "\n",
    "    for train_data, train_label in tqdm(train_dataloader):\n",
    "\n",
    "      train_label = train_label.to(device)\n",
    "      mask = train_data['attention_mask'].squeeze(1).to(device)\n",
    "      input_id = train_data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "      optimizer.zero_grad() # Reset gradients\n",
    "      loss, logits = model(input_id, mask, train_label) # Forward pass\n",
    "\n",
    "      # Calculate accuracy\n",
    "      for i in range(logits.shape[0]):\n",
    "\n",
    "        logits_clean = logits[i][train_label[i] != -100]\n",
    "        label_clean = train_label[i][train_label[i] != -100]\n",
    "\n",
    "        predictions = logits_clean.argmax(dim=1)\n",
    "        acc = (predictions == label_clean).float().mean()\n",
    "        total_acc_train += acc\n",
    "        total_loss_train += loss.item()\n",
    "\n",
    "      loss.backward()   # Backward pass\n",
    "      optimizer.step()  # Update weights\n",
    "\n",
    "    model.eval() # Set model to evaluation mode\n",
    "\n",
    "    total_acc_val = 0\n",
    "    total_loss_val = 0\n",
    "\n",
    "    # Lists to store predictions and true labels\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "\n",
    "    # Iterate over validation set\n",
    "    for val_data, val_label in val_dataloader:\n",
    "\n",
    "      val_label = val_label.to(device)\n",
    "      mask = val_data['attention_mask'].squeeze(1).to(device)\n",
    "      input_id = val_data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "      loss, logits = model(input_id, mask, val_label)\n",
    "\n",
    "      # Calculate accuracy\n",
    "      for i in range(logits.shape[0]):\n",
    "\n",
    "        logits_clean = logits[i][val_label[i] != -100]\n",
    "        label_clean = val_label[i][val_label[i] != -100]\n",
    "\n",
    "        predictions = logits_clean.argmax(dim=1)\n",
    "        acc = (predictions == label_clean).float().mean()\n",
    "        total_acc_val += acc\n",
    "        total_loss_val += loss.item()\n",
    "\n",
    "        # Collect predictions and true labels for F1 score calculation\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_true_labels.append(label_clean.cpu().numpy())\n",
    "\n",
    "    # Flatten the lists\n",
    "    all_predictions = np.concatenate(all_predictions)\n",
    "    all_true_labels = np.concatenate(all_true_labels)\n",
    "\n",
    "    # Calculate macro-averaged F1 score\n",
    "    f1 = f1_score(all_true_labels, all_predictions, average='macro')\n",
    "\n",
    "    val_accuracy = total_acc_val / len(val_ids)\n",
    "    val_loss = total_loss_val / len(val_ids)\n",
    "    train_accuracy = total_acc_train / len(train_labels)\n",
    "    train_loss = total_loss_train / len(train_labels)\n",
    "\n",
    "    print(\n",
    "        f'Epochs: {epoch_num + 1} | Loss: {train_loss: .3f} | Accuracy: {train_accuracy: .3f} | Val_Loss: {val_loss: .3f} | Accuracy: {val_accuracy: .3f} | F1-SCORE: {f1: .3f}')\n",
    "    \n",
    "    # Save checkpoints\n",
    "    checkpoints['epoch'].append(epoch_num)\n",
    "    checkpoints['model_state_dict'].append(model.state_dict())\n",
    "    checkpoints['optimizer_state_dict'].append(optimizer.state_dict())\n",
    "    checkpoints['loss'].append(val_loss)\n",
    "    checkpoints['accuracy'].append(val_accuracy)\n",
    "  \n",
    "  torch.save(checkpoints, './fine_tuned_checkpoints')\n",
    "\n",
    "\n",
    "\n",
    "model = EntityNamingModel()\n",
    "train_loop(model, train_ids, train_labels, val_ids, val_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa (A Robustly Optimized BERT Pretraining Approach)\n",
    "- **Developer**: Facebook AI\n",
    "- **Release Year**: 2019\n",
    "- **Key Features**:\n",
    "  - An optimized version of BERT with changes in pretraining procedures.\n",
    "  - Removes the NSP task and dynamically changes the masking pattern during the pretraining phase.\n",
    "  - Trained with larger mini-batches and learning rates.\n",
    "- **Applications**: More effective than BERT on several NLP benchmarks and tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoBERTa specific globals\n",
    "model_name = \"xlm-roberta-large-finetuned-conll03-german\"\n",
    "model_source = \"./RoBERTa_for_ebay/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jt/_0qr5jl901x7k02627tzb28h0000gn/T/ipykernel_13209/1732769749.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_entities['Tokenized_Length'] = df_entities['Token'].apply(lambda x: len(tokenizer.tokenize(x)))\n"
     ]
    }
   ],
   "source": [
    "train_seq = df['Title'].unique().tolist()\n",
    "\n",
    "df_entities['Tokenized_Length'] = df_entities['Token'].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "\n",
    "# Group the DataFrame by 'Record Number'\n",
    "grouped_entities = df_entities.groupby('Record Number')\n",
    "\n",
    "# Initialize a numpy array with zeros for label ids\n",
    "token_labels = np.zeros(shape=(len(train_seq), max_seq), dtype=np.int32)\n",
    "\n",
    "for i in range(5000):\n",
    "    if str(i + 1) in grouped_entities.groups:\n",
    "        curr_entities = grouped_entities.get_group(str(i + 1))\n",
    "        pointer = 1\n",
    "        for _, row in curr_entities.iterrows():\n",
    "            token_len = row['Tokenized_Length']\n",
    "            token_labels[i, pointer:(pointer + token_len)] = np.array([voc_map[row['mod_Tag']]])\n",
    "            pointer += token_len\n",
    "\n",
    "# Split data into train and validation sets\n",
    "train_ids, val_ids, train_labels, val_labels = train_test_split(train_seq, token_labels, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1188/1188 [03:08<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Loss:  0.440 | Accuracy:  0.878 | Val_Loss:  0.426 | Accuracy:  0.881 | F1-SCORE:  0.204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1188/1188 [03:06<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Loss:  0.429 | Accuracy:  0.881 | Val_Loss:  0.415 | Accuracy:  0.887 | F1-SCORE:  0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1188/1188 [03:04<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Loss:  0.419 | Accuracy:  0.884 | Val_Loss:  0.403 | Accuracy:  0.889 | F1-SCORE:  0.221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1188/1188 [03:02<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Loss:  0.408 | Accuracy:  0.886 | Val_Loss:  0.393 | Accuracy:  0.892 | F1-SCORE:  0.226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1188/1188 [02:58<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Loss:  0.400 | Accuracy:  0.888 | Val_Loss:  0.390 | Accuracy:  0.892 | F1-SCORE:  0.239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1188/1188 [02:58<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Loss:  0.392 | Accuracy:  0.890 | Val_Loss:  0.376 | Accuracy:  0.895 | F1-SCORE:  0.246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1188/1188 [02:59<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Loss:  0.383 | Accuracy:  0.892 | Val_Loss:  0.371 | Accuracy:  0.895 | F1-SCORE:  0.248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1188/1188 [03:00<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Loss:  0.376 | Accuracy:  0.893 | Val_Loss:  0.369 | Accuracy:  0.898 | F1-SCORE:  0.259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1188/1188 [02:59<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Loss:  0.368 | Accuracy:  0.895 | Val_Loss:  0.361 | Accuracy:  0.899 | F1-SCORE:  0.259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1188/1188 [02:59<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Loss:  0.362 | Accuracy:  0.897 | Val_Loss:  0.355 | Accuracy:  0.902 | F1-SCORE:  0.267\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "LEARNING_RATE = 5e-4\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# Load pre_trained configuration\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels = len(voc_map),\n",
    "    id2label = rev_map,\n",
    "    label2id = voc_map\n",
    ")\n",
    "\n",
    "# Train Loop\n",
    "def train_loop(model, train_ids, train_labels, val_ids, val_labels):\n",
    "  # Initialize a dictionary to store checkpoints in training\n",
    "  checkpoints = {\n",
    "    'epoch': [],\n",
    "    'model_state_dict': [],\n",
    "    'optimizer_state_dict': [],\n",
    "    'loss': [],\n",
    "    'accuracy': [],\n",
    "  }\n",
    "\n",
    "  train_dataset = DataSeq(train_ids, train_labels)\n",
    "  val_dataset = DataSeq(val_ids, val_labels)\n",
    "\n",
    "  train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "  val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "  device = torch.device(\"mps\") # Set device to Metal Performance Shaders\n",
    "\n",
    "  # Initialize optimizer\n",
    "  optimizer = SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "  for epoch_num in range(EPOCHS):\n",
    "\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "    model.to(device)  # Move model to GPU\n",
    "    model.train()     # Set model to training mode\n",
    "\n",
    "    for train_data, train_label in tqdm(train_dataloader):\n",
    "\n",
    "      train_label = train_label.to(device)\n",
    "      mask = train_data['attention_mask'].squeeze(1).to(device)\n",
    "      input_id = train_data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "      optimizer.zero_grad()                             # Reset gradients\n",
    "      loss, logits = model(input_id, mask, train_label) # Forward pass\n",
    "\n",
    "      # Calculate accuracy\n",
    "      for i in range(logits.shape[0]):\n",
    "\n",
    "        logits_clean = logits[i][train_label[i] != -100]\n",
    "        label_clean = train_label[i][train_label[i] != -100]\n",
    "\n",
    "        predictions = logits_clean.argmax(dim=1)\n",
    "        acc = (predictions == label_clean).float().mean()\n",
    "        total_acc_train += acc\n",
    "        total_loss_train += loss.item()\n",
    "\n",
    "      loss.backward()   # Backward pass\n",
    "      optimizer.step()  # Update weights\n",
    "\n",
    "    model.eval() # Set model to evaluation mode\n",
    "\n",
    "    total_acc_val = 0\n",
    "    total_loss_val = 0\n",
    "\n",
    "    # Lists to store predictions and true labels\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "\n",
    "\n",
    "    # Iterate over validation set\n",
    "    for val_data, val_label in val_dataloader:\n",
    "\n",
    "      val_label = val_label.to(device)\n",
    "      mask = val_data['attention_mask'].squeeze(1).to(device)\n",
    "      input_id = val_data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "      loss, logits = model(input_id, mask, val_label)\n",
    "\n",
    "      # Calculate accuracy\n",
    "      for i in range(logits.shape[0]):\n",
    "\n",
    "        logits_clean = logits[i][val_label[i] != -100]\n",
    "        label_clean = val_label[i][val_label[i] != -100]\n",
    "\n",
    "        predictions = logits_clean.argmax(dim=1)\n",
    "        acc = (predictions == label_clean).float().mean()\n",
    "        total_acc_val += acc\n",
    "        total_loss_val += loss.item()\n",
    "\n",
    "        # Collect predictions and true labels for F1 score calculation\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_true_labels.append(label_clean.cpu().numpy())\n",
    "\n",
    "    # Flatten the lists\n",
    "    all_predictions = np.concatenate(all_predictions)\n",
    "    all_true_labels = np.concatenate(all_true_labels)\n",
    "\n",
    "    # Calculate macro-averaged F1 score\n",
    "    f1 = f1_score(all_true_labels, all_predictions, average='macro')\n",
    "\n",
    "    val_accuracy = total_acc_val / len(val_ids)\n",
    "    val_loss = total_loss_val / len(val_ids)\n",
    "    train_accuracy = total_acc_train / len(train_labels)\n",
    "    train_loss = total_loss_train / len(train_labels)\n",
    "\n",
    "    print(\n",
    "        f'Epochs: {epoch_num + 1} | Loss: {train_loss: .3f} | Accuracy: {train_accuracy: .3f} | Val_Loss: {val_loss: .3f} | Accuracy: {val_accuracy: .3f} | F1-SCORE: {f1: .3f}')\n",
    "      \n",
    "    # Save checkpoints\n",
    "    checkpoints['epoch'].append(epoch_num)\n",
    "    checkpoints['model_state_dict'].append(model.state_dict())\n",
    "    checkpoints['optimizer_state_dict'].append(optimizer.state_dict())\n",
    "    checkpoints['loss'].append(val_loss)\n",
    "    checkpoints['accuracy'].append(val_accuracy)\n",
    "  \n",
    "  torch.save(checkpoints, './fine_tuned_checkpoints')\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_source,\n",
    "    num_labels = len(voc_map),\n",
    "    id2label = rev_map,\n",
    "    label2id = voc_map\n",
    ")\n",
    "\n",
    "# model = RobertaNamingModel(config=config)\n",
    "train_loop(model, train_ids, train_labels, val_ids, val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EntityNamingModel()\n",
    "optimizer = SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "model_number = 1\n",
    "checkpoint = torch.load(\"./fine_tuned_checkpoints\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'][model_number])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'][model_number])\n",
    "epoch = checkpoint['epoch'][model_number]\n",
    "loss = checkpoint['loss'][model_number]\n",
    "accuracy = checkpoint['accuracy'][model_number]\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_VAL = \"Listing_Titles.tsv\"\n",
    "dfVAL = pd.read_csv(data_path_VAL, sep=\"\\t\", dtype=str, keep_default_na=False, na_values=[\"\"], quoting=csv.QUOTE_NONE, skiprows = lambda x : x > 30000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 500/500 [00:00<00:00, 13140.21texts/s]\n",
      "Inferencing: 100%|██████████| 16/16 [00:04<00:00,  3.35batches/s]\n"
     ]
    }
   ],
   "source": [
    "orig = dfVAL['Title'].to_list()[29500:]#[5000:]\n",
    "predictions = []\n",
    "device = torch.device(\"mps\")\n",
    "inferDat = DataInf(orig)\n",
    "load = DataLoader(inferDat, batch_size=32)\n",
    "for data in tqdm(load, desc=\"Inferencing\", unit=\"batches\"):\n",
    "\n",
    "    mask = data['attention_mask'].squeeze(1).to(device)\n",
    "    input_id = data['input_ids'].squeeze(1).to(device)\n",
    "    model.to(device)\n",
    "\n",
    "    logits = model.forward(input_id, mask, None)[1]\n",
    "    for i in range(logits.shape[0]):\n",
    "        logit = logits[i]\n",
    "        predictions.append(logit.argmax(dim=1).cpu())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁Adidas',\n",
       " '▁Ultra',\n",
       " '▁Boost',\n",
       " '▁X',\n",
       " '▁3',\n",
       " 'D',\n",
       " '▁S',\n",
       " '▁Stella',\n",
       " '▁Damen',\n",
       " '▁Sneaker',\n",
       " '▁Lauf',\n",
       " 'schuhe',\n",
       " '▁Turn',\n",
       " 'schuhe',\n",
       " '▁G',\n",
       " '28',\n",
       " '3',\n",
       " '36',\n",
       " '▁N',\n",
       " 'EU']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(orig[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 500/25000 [00:00<00:01, 15627.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Adidas ',\n",
       " 'Ultra ',\n",
       " 'Boost ',\n",
       " 'X ',\n",
       " '3D ',\n",
       " 'S ',\n",
       " 'Stella ',\n",
       " 'Damen ',\n",
       " 'Sneaker ',\n",
       " 'Laufschuhe ',\n",
       " 'Turnschuhe ',\n",
       " 'G28336 ',\n",
       " '',\n",
       " 'adidas ',\n",
       " 'Adilette ',\n",
       " 'Boost ',\n",
       " 'Slide ',\n",
       " 'Sandal ',\n",
       " ', ',\n",
       " 'Black ',\n",
       " ', ',\n",
       " 'Size ',\n",
       " '4.0 ',\n",
       " 'nv0t ',\n",
       " '',\n",
       " 'Diesel ',\n",
       " 'BIKKREN ',\n",
       " 'Herrenschuhe ',\n",
       " 'Turnschuhe ',\n",
       " 'Leder ',\n",
       " 'Freizeit ',\n",
       " 'Men ',\n",
       " 'Sneaker ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " '',\n",
       " 'Asics ',\n",
       " 'Dynaflyte ',\n",
       " '3 ',\n",
       " '1011A253001 ',\n",
       " 'schwarz ',\n",
       " '',\n",
       " 'Nike ',\n",
       " 'Air ',\n",
       " 'Force ',\n",
       " '1 ',\n",
       " 'Low ',\n",
       " 'Damen ',\n",
       " '! ',\n",
       " 'CQ7511 ',\n",
       " '071 ',\n",
       " '! ',\n",
       " 'US ',\n",
       " '8 ',\n",
       " 'EU ',\n",
       " '39 ',\n",
       " '! ',\n",
       " '25 ',\n",
       " 'CM ',\n",
       " '! ',\n",
       " 'NEU ',\n",
       " '',\n",
       " 'nike ',\n",
       " 'air ',\n",
       " 'force ',\n",
       " '1 ',\n",
       " '07 ',\n",
       " 'lv8 ',\n",
       " '3 ',\n",
       " 'ungetragen ',\n",
       " '',\n",
       " 'Nike ',\n",
       " 'Sneakers ',\n",
       " 'Damen ',\n",
       " 'Freizeitschuhe ',\n",
       " 'Turnschuhe ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " 'DE ',\n",
       " '37.5 ',\n",
       " 'kein ',\n",
       " 'Etike ',\n",
       " '... ',\n",
       " '#da ',\n",
       " '',\n",
       " 'Converse ',\n",
       " 'Sneakers ',\n",
       " 'Damen ',\n",
       " 'Freizeitschuhe ',\n",
       " 'Turnschuhe ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " 'DE ',\n",
       " '40 ',\n",
       " 'kein ',\n",
       " 'Eti ',\n",
       " '... ',\n",
       " '# ',\n",
       " '',\n",
       " 'Adidas ',\n",
       " 'Sneaker ',\n",
       " 'Größe ',\n",
       " '40 ',\n",
       " 'schwarz ',\n",
       " 'all ',\n",
       " 'black ',\n",
       " 'nmd ',\n",
       " 'Racer ',\n",
       " 'Ultra ',\n",
       " 'Boost ',\n",
       " 'Turnschuhe ',\n",
       " '',\n",
       " 'Nike ',\n",
       " 'Air ',\n",
       " 'Jordan ',\n",
       " '4 ',\n",
       " 'Toro ',\n",
       " 'Bravo ',\n",
       " 'size ',\n",
       " 'US ',\n",
       " '11,5 ',\n",
       " 'Grösse ',\n",
       " '45,5 ',\n",
       " 'Neu ',\n",
       " 'New ',\n",
       " 'DS ',\n",
       " 'Cavs ',\n",
       " '1 ',\n",
       " '2 ',\n",
       " '3 ',\n",
       " '',\n",
       " 'S.Oliver ',\n",
       " 'Schuhe ',\n",
       " 'Sneaker ',\n",
       " 'Damen ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " '',\n",
       " 'Tamaris ',\n",
       " 'Sneaker ',\n",
       " 'Schnürschuh ',\n",
       " '',\n",
       " 'Sneakers ',\n",
       " 'Hummel ',\n",
       " 'Unisex ',\n",
       " 'EUR ',\n",
       " '41 ',\n",
       " '* ',\n",
       " 'superleicht ',\n",
       " '',\n",
       " 'MERRELL ',\n",
       " 'Vapor ',\n",
       " 'Glove ',\n",
       " '3 ',\n",
       " 'Luna ',\n",
       " 'LTR ',\n",
       " 'Barefoot ',\n",
       " 'Sneaker ',\n",
       " 'Turnschuhe ',\n",
       " 'Schuhe ',\n",
       " 'Damen ',\n",
       " '',\n",
       " 'Adidas ',\n",
       " 'Schuhe ',\n",
       " 'Running ',\n",
       " 'Galaxy ',\n",
       " 'Incision ',\n",
       " 'M ',\n",
       " 'Laufschuhe ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " '10 ',\n",
       " '44 ',\n",
       " '2/3 ',\n",
       " 'Neu ',\n",
       " '* ',\n",
       " '',\n",
       " 'Nike ',\n",
       " 'basketballschuhe ',\n",
       " 'lunarlon ',\n",
       " 'Gr ',\n",
       " '',\n",
       " 'adidas ',\n",
       " 'Originals ',\n",
       " 'Sneakers ',\n",
       " 'Damen ',\n",
       " 'Freizeitschuhe ',\n",
       " 'Turnschuhe ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " 'UK ',\n",
       " '6 ',\n",
       " '( ',\n",
       " '... ',\n",
       " '# ',\n",
       " '',\n",
       " 'DAMEN ',\n",
       " 'SCHUHE ',\n",
       " '44966 ',\n",
       " 'SNEAKERS ',\n",
       " 'HELLBRAUN ',\n",
       " '37 ',\n",
       " '',\n",
       " 'Adidas ',\n",
       " 'Beyonce ',\n",
       " 'Superstar ',\n",
       " 'Leder ',\n",
       " 'Plateau ',\n",
       " 'Turnschuhe ',\n",
       " 'Sneakers ',\n",
       " 'Größe ',\n",
       " '',\n",
       " 'converse ',\n",
       " 'all ',\n",
       " 'star ',\n",
       " 'Schuh ',\n",
       " '',\n",
       " 'Nike ',\n",
       " 'Turnschuhe ',\n",
       " 'Damen ',\n",
       " 'Größe ',\n",
       " '',\n",
       " 'DAMEN ',\n",
       " 'SCHUHE ',\n",
       " '128914 ',\n",
       " 'SNEAKERS ',\n",
       " 'SILBER ',\n",
       " '39 ',\n",
       " '',\n",
       " 'Adidas ',\n",
       " 'Superstar ',\n",
       " 'Supershell ',\n",
       " 'Pharrell ',\n",
       " 'Williams ',\n",
       " '42 ',\n",
       " '2/3 ',\n",
       " 'UK ',\n",
       " '8.5 ',\n",
       " 'S83368 ',\n",
       " '* ',\n",
       " 'NEU ',\n",
       " '',\n",
       " 'adidas ',\n",
       " 'Consortium ',\n",
       " 'Crazy ',\n",
       " '1 ',\n",
       " 'ADV ',\n",
       " '-- ',\n",
       " '2019 ',\n",
       " '- ',\n",
       " 'US ',\n",
       " '9,5 ',\n",
       " '-- ',\n",
       " '',\n",
       " 'Sneaker ',\n",
       " 'Turnschuhe ',\n",
       " 'von ',\n",
       " 'Candice ',\n",
       " 'Cooper ',\n",
       " ', ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " '38 ',\n",
       " ', ',\n",
       " 'braun ',\n",
       " ', ',\n",
       " '',\n",
       " 'Nicola ',\n",
       " 'Barbato ',\n",
       " 'Beatles ',\n",
       " 'Sfilato ',\n",
       " 'bergschuhe ',\n",
       " 'gr ',\n",
       " '42 ',\n",
       " 'HERRENSCHUHE ',\n",
       " 'FASHION ',\n",
       " '',\n",
       " 'Puma ',\n",
       " 'Roma ',\n",
       " 'Ripstop ',\n",
       " 'Clear ',\n",
       " 'Damen ',\n",
       " 'US ',\n",
       " '10 ',\n",
       " 'Weiß ',\n",
       " 'Laufschuh ',\n",
       " '4132 ',\n",
       " 'EU ',\n",
       " '',\n",
       " 'ECCO ',\n",
       " 'Sneaker ',\n",
       " 'komfortabel ',\n",
       " 'Turnschuh ',\n",
       " 'bequem ',\n",
       " 'und ',\n",
       " 'fußgerecht ',\n",
       " 'Taubenblau ',\n",
       " '40/6 ',\n",
       " ',5 ',\n",
       " '',\n",
       " 'Herrenschuhe ',\n",
       " 'Puma ',\n",
       " ', ',\n",
       " 'hellbraun ',\n",
       " '/ ',\n",
       " 'beige ',\n",
       " ', ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " '44 ',\n",
       " ', ',\n",
       " 'wie ',\n",
       " '',\n",
       " 'S.OLIVER ',\n",
       " 'SNEAKER ',\n",
       " 'IM ',\n",
       " 'VINTAGE-LOOK ',\n",
       " 'GR ',\n",
       " '.37 ',\n",
       " 'SILBER-GRAU ',\n",
       " '',\n",
       " 'Converse ',\n",
       " 'All ',\n",
       " 'Star ',\n",
       " 'Chuck ',\n",
       " 'Taylor ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " '40 ',\n",
       " '- ',\n",
       " 'NEU ',\n",
       " '',\n",
       " 'Converse ',\n",
       " 'Chucks ',\n",
       " ', ',\n",
       " 'Jack ',\n",
       " 'Purcell ',\n",
       " ', ',\n",
       " 'Leder ',\n",
       " ', ',\n",
       " 'Blau ',\n",
       " ', ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " '46 ',\n",
       " '** ',\n",
       " 'NEU ',\n",
       " '',\n",
       " 'Nike ',\n",
       " 'Blazer ',\n",
       " 'High ',\n",
       " 'Sneaker ',\n",
       " 'Pink ',\n",
       " '/ ',\n",
       " 'Rosa ',\n",
       " 'US8 ',\n",
       " 'UK5 ',\n",
       " ',5 ',\n",
       " 'EUR39 ',\n",
       " '',\n",
       " '* ',\n",
       " 'ASICS ',\n",
       " '* ',\n",
       " 'Sneaker ',\n",
       " 'Amplica ',\n",
       " 'silbergrau ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " '40,5 ',\n",
       " 'Gym ',\n",
       " '',\n",
       " '2005 ',\n",
       " 'Nike ',\n",
       " 'Damen ',\n",
       " 'Shox ',\n",
       " 'Revolution ',\n",
       " 'Turnschuhe ',\n",
       " ', ',\n",
       " '311233-114 ',\n",
       " '. ',\n",
       " 'Größe ',\n",
       " 'W9US ',\n",
       " '/ ',\n",
       " '6,5 ',\n",
       " '',\n",
       " 'SKECHERS ',\n",
       " 'Damen-Sneaker ',\n",
       " 'Quick ',\n",
       " 'fit ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " '',\n",
       " 'Converse ',\n",
       " 'All ',\n",
       " 'Star ',\n",
       " 'Hi ',\n",
       " 'High ',\n",
       " 'Chucks ',\n",
       " ', ',\n",
       " 'Sneaker ',\n",
       " ', ',\n",
       " 'Boots ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " '38 ',\n",
       " '; ',\n",
       " '5 ',\n",
       " '1/2 ',\n",
       " 'grau ',\n",
       " '',\n",
       " 'Tolle ',\n",
       " 'Farbe ',\n",
       " ': ',\n",
       " 'Schuhe ',\n",
       " ', ',\n",
       " 'Grünbein ',\n",
       " ', ',\n",
       " 'Louis ',\n",
       " 'Punkt ',\n",
       " ', ',\n",
       " 'Leder ',\n",
       " ', ',\n",
       " 'Größe ',\n",
       " '39 ',\n",
       " ', ',\n",
       " '',\n",
       " 'Nike ',\n",
       " 'Air ',\n",
       " 'Max ',\n",
       " ', ',\n",
       " 'Damen ',\n",
       " ', ',\n",
       " 'Blau ',\n",
       " ', ',\n",
       " 'Größe ',\n",
       " '40 ',\n",
       " '( ',\n",
       " 'passt ',\n",
       " 'auch ',\n",
       " 'in ',\n",
       " '41 ',\n",
       " '',\n",
       " 'New ',\n",
       " 'Balance ',\n",
       " 'U420KW1 ',\n",
       " 'neaker ',\n",
       " '\" ',\n",
       " 'NEU ',\n",
       " '\" ',\n",
       " ', ',\n",
       " 'blau ',\n",
       " '/ ',\n",
       " 'weiß ',\n",
       " ', ',\n",
       " '38 ',\n",
       " 'EUR ',\n",
       " '· ',\n",
       " '5,5 ',\n",
       " '',\n",
       " '*** ',\n",
       " 'ESPRIT ',\n",
       " 'Sneaker ',\n",
       " ', ',\n",
       " 'Größe ',\n",
       " '39 ',\n",
       " ', ',\n",
       " 'TOP ',\n",
       " ', ',\n",
       " 'TOP ',\n",
       " ', ',\n",
       " 'TOP ',\n",
       " '',\n",
       " 'Prada ',\n",
       " 'Sport ',\n",
       " 'Herren ',\n",
       " 'Sneaker ',\n",
       " 'Car ',\n",
       " 'Shoe ',\n",
       " 'weiss ',\n",
       " '/ ',\n",
       " 'rot ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " '',\n",
       " 'NIKE ',\n",
       " 'PRESTO ',\n",
       " 'LIM.EDITION ',\n",
       " '!!! ',\n",
       " '',\n",
       " 'Birkenstock ',\n",
       " 'Madrid ',\n",
       " '1008493 ',\n",
       " 'rosa ',\n",
       " '',\n",
       " 'Converse ',\n",
       " 'Sneakers ',\n",
       " 'Damen ',\n",
       " 'Freizeitschuhe ',\n",
       " 'Turnschuhe ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " 'DE ',\n",
       " '39.5 ',\n",
       " 'kein ',\n",
       " 'E. ',\n",
       " '. ',\n",
       " '. ',\n",
       " '#f ',\n",
       " '',\n",
       " 'Lacoste ',\n",
       " 'Sneakers ',\n",
       " 'Herren ',\n",
       " 'Freizeitschuhe ',\n",
       " 'Turnschuhe ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " 'DE ',\n",
       " '42 ',\n",
       " 'weiß ',\n",
       " '# ',\n",
       " '',\n",
       " 'Nike ',\n",
       " 'Air ',\n",
       " 'Max ',\n",
       " '1 ',\n",
       " 'Premium ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " '44.5 ',\n",
       " '10.5 ',\n",
       " '512033-040 ',\n",
       " 'Classic ',\n",
       " '90 ',\n",
       " '95 ',\n",
       " '98 ',\n",
       " 'beige ',\n",
       " 'blau ',\n",
       " '',\n",
       " 'JUICY ',\n",
       " 'Damen ',\n",
       " 'BOOTS ',\n",
       " '/ ',\n",
       " 'Schuhe ',\n",
       " 'Gr ',\n",
       " '',\n",
       " 'Nike ',\n",
       " 'Airmax ',\n",
       " 'Kinderschuhe ',\n",
       " 'Größe ',\n",
       " '',\n",
       " 'Converse ',\n",
       " 'Sneakers ',\n",
       " 'Damen ',\n",
       " 'Freizeitschuhe ',\n",
       " 'Turnschuhe ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " 'DE ',\n",
       " '40 ',\n",
       " 'kein ',\n",
       " 'Eti ',\n",
       " '... ',\n",
       " '# ',\n",
       " '',\n",
       " 'Damen ',\n",
       " 'Sneakers ',\n",
       " 'Nike ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " 'DE ',\n",
       " '42 ',\n",
       " 'schwarz ',\n",
       " '# ',\n",
       " '',\n",
       " '100 ',\n",
       " '% ',\n",
       " 'original ',\n",
       " 'Adidas ',\n",
       " 'Iniki ',\n",
       " 'I ',\n",
       " '5923 ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " '',\n",
       " 'Timberland ',\n",
       " 'Earthkeepers ',\n",
       " 'Turnschuhe ',\n",
       " 'gr ',\n",
       " '41 ',\n",
       " '',\n",
       " 'Asics ',\n",
       " 'Tiger ',\n",
       " 'Gel-Lyte ',\n",
       " 'III ',\n",
       " 'Herren-Sneaker ',\n",
       " 'Freizeit ',\n",
       " 'Schuhe ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " '',\n",
       " 'Curry ',\n",
       " '2 ',\n",
       " 'Basketballschuhe ',\n",
       " 'Grösse ',\n",
       " '47,5 ',\n",
       " 'US ',\n",
       " 'Size ',\n",
       " '',\n",
       " 'Adidas ',\n",
       " 'Damen ',\n",
       " 'Sneaker ',\n",
       " 'S76536 ',\n",
       " 'Superstar ',\n",
       " '80 ',\n",
       " \"' \",\n",
       " 's ',\n",
       " 'Primeknit ',\n",
       " 'Slip-On ',\n",
       " 'weiß ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " '36 ',\n",
       " 'UK ',\n",
       " '',\n",
       " 'Adidas ',\n",
       " '\" ',\n",
       " 'Forest ',\n",
       " 'Hills ',\n",
       " 'Round ',\n",
       " 'w ',\n",
       " '\" ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " 'UK ',\n",
       " '5 ',\n",
       " '',\n",
       " 'Sneaker ',\n",
       " '\" ',\n",
       " 'Paul ',\n",
       " 'Green ',\n",
       " '\" ',\n",
       " 'WIE ',\n",
       " 'NEU ',\n",
       " ', ',\n",
       " 'Größe ',\n",
       " '4 ',\n",
       " '1/2 ',\n",
       " '( ',\n",
       " '37,5 ',\n",
       " ') ',\n",
       " ', ',\n",
       " 'grau ',\n",
       " '',\n",
       " 'Convers ',\n",
       " 'All ',\n",
       " 'Stars ',\n",
       " ', ',\n",
       " 'grün ',\n",
       " ', ',\n",
       " 'Gr.UK ',\n",
       " '',\n",
       " 'Nike ',\n",
       " 'BLAZER ',\n",
       " 'MID ',\n",
       " 'VINTAGE ',\n",
       " '( ',\n",
       " 'TD ',\n",
       " ') ',\n",
       " 'Schuhe ',\n",
       " 'Sneaker ',\n",
       " 'Low ',\n",
       " 'Beige ',\n",
       " 'Jungen ',\n",
       " '/ ',\n",
       " 'Mädchen ',\n",
       " 'Frühling ',\n",
       " '/ ',\n",
       " '',\n",
       " 'MENBUR ',\n",
       " 'Sneakers ',\n",
       " 'Damen ',\n",
       " 'Freizeitschuhe ',\n",
       " 'Turnschuhe ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " 'DE ',\n",
       " '40 ',\n",
       " 'kein ',\n",
       " 'Etike ',\n",
       " '... ',\n",
       " '# ',\n",
       " '',\n",
       " 'TRENDY ',\n",
       " 'HERREN ',\n",
       " 'SCHUHE ',\n",
       " '120810 ',\n",
       " 'SPORTSCHUHE ',\n",
       " 'ROT ',\n",
       " '42 ',\n",
       " '',\n",
       " 'XIT ',\n",
       " ', ',\n",
       " 'Slipper ',\n",
       " ', ',\n",
       " 'Größe ',\n",
       " ': ',\n",
       " '',\n",
       " 'Skechers ',\n",
       " 'Sneaker ',\n",
       " 'Schuhe ',\n",
       " 'Sportschuhe ',\n",
       " 'Rot ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " '39 ',\n",
       " 'UK ',\n",
       " '',\n",
       " 'Adidas ',\n",
       " 'Tango ',\n",
       " 'Tee ',\n",
       " 'AZ9719 ',\n",
       " 'Schwarz ',\n",
       " 'Kurzer ',\n",
       " '',\n",
       " 'OSIRIS ',\n",
       " 'EXACT-SCIENCE ',\n",
       " 'Sneaker ',\n",
       " 'Schuhe ',\n",
       " '43 ',\n",
       " 'Schwarz ',\n",
       " 'High ',\n",
       " 'supra ',\n",
       " 'skate ',\n",
       " 'dc ',\n",
       " 'dvs ',\n",
       " 'wie ',\n",
       " 'NEU ',\n",
       " '',\n",
       " 'Herren ',\n",
       " 'Sneakers ',\n",
       " 'Sportschuhe ',\n",
       " 'Schnürer ',\n",
       " 'Lederoptik ',\n",
       " 'Schuhe ',\n",
       " '98917 ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " '40-45 ',\n",
       " '',\n",
       " 'Damen ',\n",
       " 'Schuhe ',\n",
       " '. ',\n",
       " 'Gruen ',\n",
       " '. ',\n",
       " 'Gr ',\n",
       " '',\n",
       " '960964 ',\n",
       " 'VANS ',\n",
       " 'Slip ',\n",
       " 'On ',\n",
       " 'Checkerboard ',\n",
       " 'espresso ',\n",
       " 'true ',\n",
       " 'white ',\n",
       " '',\n",
       " 'Adidas ',\n",
       " ', ',\n",
       " 'Sportschuhe ',\n",
       " ', ',\n",
       " 'Größe ',\n",
       " ': ',\n",
       " '42 ',\n",
       " ', ',\n",
       " 'Schwarz ',\n",
       " '/ ',\n",
       " '',\n",
       " 'MBT ',\n",
       " 'Schuhe ',\n",
       " 'Tataga ',\n",
       " 'C ',\n",
       " 'Chill ',\n",
       " 'Gr ',\n",
       " '39 ',\n",
       " 'Damen ',\n",
       " 'Original ',\n",
       " 'Barfuss ',\n",
       " 'Modell ',\n",
       " 'Chapa ',\n",
       " '',\n",
       " 'K1X ',\n",
       " 'gk ',\n",
       " '3000 ',\n",
       " 'le ',\n",
       " 'Groesse ',\n",
       " '',\n",
       " 'Adidas ',\n",
       " 'Missy ',\n",
       " 'Game ',\n",
       " 'Respect ',\n",
       " 'Me ',\n",
       " 'Lady ',\n",
       " 'Sneakers ',\n",
       " '37 ',\n",
       " '1/3 ',\n",
       " 'Neu ',\n",
       " 'Rare ',\n",
       " 'Weiß ',\n",
       " 'Blau ',\n",
       " 'Eqt ',\n",
       " '',\n",
       " 'Converse ',\n",
       " 'Chucks ',\n",
       " 'Gr ',\n",
       " '43 ',\n",
       " 'Sneakers ',\n",
       " 'Schuhe ',\n",
       " 'Punk ',\n",
       " '',\n",
       " 'Frye ',\n",
       " 'Tegan ',\n",
       " 'Low ',\n",
       " 'Lace ',\n",
       " 'Damen ',\n",
       " 'Beige ',\n",
       " 'Leder ',\n",
       " 'Turnschuhe ',\n",
       " '',\n",
       " 'Nike ',\n",
       " 'Air ',\n",
       " 'JORDAN ',\n",
       " '6 ',\n",
       " 'Low ',\n",
       " '| ',\n",
       " 'Jordan ',\n",
       " 'VI ',\n",
       " 'Low ',\n",
       " '| ',\n",
       " 'Weiss-Rot-Schwarz ',\n",
       " '| ',\n",
       " '44 ',\n",
       " '| ',\n",
       " '28cm ',\n",
       " '| ',\n",
       " '',\n",
       " 'Mistral ',\n",
       " 'Turnschuh ',\n",
       " 'Sneakers ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " '44 ',\n",
       " 'NEU ',\n",
       " '',\n",
       " 'gebraucht ',\n",
       " 'Adidas ',\n",
       " 'niedrig-Schnitt ',\n",
       " 'Turnschuhe ',\n",
       " 'Blk ',\n",
       " '',\n",
       " 'Columbia ',\n",
       " 'Sportswear ',\n",
       " 'Company ',\n",
       " 'Sneakers ',\n",
       " 'Damen ',\n",
       " 'Freizeitschuhe ',\n",
       " 'Turnschuhe ',\n",
       " '... ',\n",
       " '# ',\n",
       " '',\n",
       " 'BRANDNEU ',\n",
       " 'DAMEN ',\n",
       " 'SCHUHE ',\n",
       " '111994 ',\n",
       " 'SPORTSCHUHE ',\n",
       " 'DUNKELBLAU ',\n",
       " '',\n",
       " 'Sneaker ',\n",
       " 'Damen ',\n",
       " 'Schuhe ',\n",
       " 'TOP ',\n",
       " 'Sportschuhe ',\n",
       " 'Trainer ',\n",
       " 'Freizeitschuhe ',\n",
       " '67327 ',\n",
       " 'Weiß ',\n",
       " 'Gold ',\n",
       " '',\n",
       " 'Secondhand ',\n",
       " 'PUMA ',\n",
       " 'Han ',\n",
       " 'Kjobenhavncell ',\n",
       " 'Venom ',\n",
       " '19Ss ',\n",
       " 'Sneakers ',\n",
       " '369565-01 ',\n",
       " 'Weiß ',\n",
       " '',\n",
       " 'Puma ',\n",
       " 'Sneaker ',\n",
       " 'Turnschuh ',\n",
       " '37 ',\n",
       " 'Rosa ',\n",
       " '',\n",
       " 'Chuck ',\n",
       " 'Taylor ',\n",
       " 'Chucks ',\n",
       " 'High ',\n",
       " 'blau ',\n",
       " 'Gr ',\n",
       " '. ',\n",
       " '44 ',\n",
       " 'kaum ',\n",
       " '',\n",
       " 'Ted ',\n",
       " 'Baker ',\n",
       " 'Deekun ',\n",
       " 'Herren ',\n",
       " 'Navy ',\n",
       " 'Leder ',\n",
       " '& ',\n",
       " 'Textil ',\n",
       " 'Schuhe ',\n",
       " '',\n",
       " 'Damen ',\n",
       " 'Sneaker ',\n",
       " 'Schuhe ',\n",
       " 'Versteckt ',\n",
       " 'Keilabsatz ',\n",
       " 'Schnürschuh ',\n",
       " 'Sportschuhe ',\n",
       " '',\n",
       " 'Converse ',\n",
       " 'Chucks ',\n",
       " 'AllStar ',\n",
       " 'Slip ',\n",
       " 'On ',\n",
       " 'Black ',\n",
       " 'White ',\n",
       " '1V019 ',\n",
       " '+ ',\n",
       " 'Neu ',\n",
       " '+ ',\n",
       " 'versch ',\n",
       " '. ',\n",
       " '',\n",
       " 'Schuhe ',\n",
       " 'Nike ',\n",
       " 'schwarz ',\n",
       " 'weiss ',\n",
       " '44 ',\n",
       " 'Sneaker ',\n",
       " 'Turnschuhe ',\n",
       " 'Sport ',\n",
       " '',\n",
       " 'Puma ',\n",
       " 'Sprinterschuh ',\n",
       " 'Sneaker ',\n",
       " 'getragen ',\n",
       " 'Used ',\n",
       " 'Look ',\n",
       " 'Größe ',\n",
       " 'UK ',\n",
       " '11 ',\n",
       " 'US ',\n",
       " '12 ',\n",
       " 'Eur ',\n",
       " '46 ',\n",
       " 'Cool ',\n",
       " '',\n",
       " 'Original ',\n",
       " 'Isabel ',\n",
       " 'Marant ',\n",
       " 'sneaker ',\n",
       " 'wedges ',\n",
       " 'Etoile ',\n",
       " 'bekett ',\n",
       " '40 ',\n",
       " 'Keilabsatz ',\n",
       " '',\n",
       " 'PF ',\n",
       " 'Flyers ',\n",
       " 'Brown ',\n",
       " 'Plaid ',\n",
       " '/ ',\n",
       " 'Braun ',\n",
       " 'Grösse ',\n",
       " '',\n",
       " 'Men ',\n",
       " 'Women ',\n",
       " 'Asics ',\n",
       " 'Onitsuka ',\n",
       " 'Tiger ',\n",
       " 'Dualio ',\n",
       " 'Neu ',\n",
       " 'Gr ',\n",
       " ':37 ',\n",
       " 'light ',\n",
       " 'grey ',\n",
       " 'D6K3N ',\n",
       " '1323 ',\n",
       " ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=[]\n",
    "labels=[]\n",
    "record=[]\n",
    "for i, seq in tqdm(enumerate(predictions), total=25000):\n",
    "    tokens = tokenizer.tokenize(orig[i])\n",
    "    preds = seq.tolist()[1:]\n",
    "    curr = 0\n",
    "    for word in orig[i].split(' '):\n",
    "        words.append(word)\n",
    "        labels.append(rev_map[preds[curr]])\n",
    "        record.append(i+5001)\n",
    "        prelim_word = \"\"\n",
    "        for j in range(curr, len(tokens)):\n",
    "            tok = tokens[j]\n",
    "            if tok[0] != '▁': # Change condition to =='#' when dealing with Bert\n",
    "                prelim_word += (tok)\n",
    "            else:\n",
    "                prelim_word += (tok[1:])\n",
    "            if prelim_word == word:\n",
    "                curr = j+1\n",
    "                prelim_word += (' ')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Record Mapping: 100%|██████████| 265261/265261 [01:19<00:00, 3338.45words/s] \n"
     ]
    }
   ],
   "source": [
    "fixed_words = []\n",
    "fixed_labels = []\n",
    "fixed_records = []\n",
    "temp_string = \"\"\n",
    "\n",
    "for i in tqdm(range(len(words)), desc=\"Record Mapping\", unit=\"words\"):\n",
    "    if labels[i] != \"\":\n",
    "        temp_string = words[i]\n",
    "        for j in range(len(words[i+1:])):\n",
    "            temp = words[j+i+1]\n",
    "            if(labels[j+i+1] != \"\"):\n",
    "                break\n",
    "            else:\n",
    "                temp_string += \" \" + temp\n",
    "        fixed_words.append(temp_string)\n",
    "        fixed_labels.append(labels[i])\n",
    "        fixed_records.append(record[i])\n",
    "\n",
    "dffer = {'Record Number' : fixed_records,\n",
    "         'Aspect Name' : fixed_labels,\n",
    "         'Aspect Value' : fixed_words,\n",
    "        }\n",
    "\n",
    "dffer = pd.DataFrame(dffer)\n",
    "\n",
    "dffer.to_csv('sub9.tsv', sep=\"\\t\", header=None, index=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {'Record Number' : record,\n",
    "         'Aspect Name' : labels,\n",
    "         'Aspect Value' : words,\n",
    "        }\n",
    "\n",
    "test = pd.DataFrame(test)\n",
    "\n",
    "\n",
    "dffer = {'Record Number' : fixed_records,\n",
    "         'Aspect Name' : fixed_labels,\n",
    "         'Aspect Value' : fixed_words,\n",
    "        }\n",
    "\n",
    "dffer = pd.DataFrame(dffer)\n",
    "\n",
    "\n",
    "dffer.to_csv('sub1.tsv', sep=\"\\t\", header=None, index=None) \n",
    "\n",
    "dffer.head(25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfVAL['check'] = dfVAL.apply(lambda row: len(tokenizer.tokenize(row['Title'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfVAL['check'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
